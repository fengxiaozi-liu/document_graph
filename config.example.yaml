llm:
  # OpenAI-compatible Chat Completions endpoint base URL.
  # Examples:
  # - https://api.openai.com
  # - https://dashscope.aliyuncs.com/compatible-mode
  base_url: "https://dashscope.aliyuncs.com/compatible-mode"
  api_key: "REPLACE_ME"
  model: "qwen-plus"
  temperature: 0.2

embedding:
  # OpenAI-compatible Embeddings endpoint base URL.
  # Can be the same as llm.base_url.
  base_url: "https://dashscope.aliyuncs.com/compatible-mode"
  api_key: "REPLACE_ME"
  model: "text-embedding-v3"
  # DashScope compatible-mode has a strict max batch size (often <= 10).
  batch_size: 10

qdrant:
  url: "http://localhost:6333"
  collection: "document_graph_chunks"
  distance: "Cosine" # Cosine | Dot | Euclid

paths:
  export_dir: "data/local_export"
  chunks_jsonl: "data/local_export/chunks/chunks.jsonl"

chunking:
  # Char-based defaults (simple + robust). Tune later if you add a tokenizer.
  target_chars: 1200
  max_chars: 2400
  overlap_chars: 200

ocr:
  # OCR for images (png/jpg/jpeg/webp). Requires Tesseract in runtime image.
  enabled: true
  # Tesseract language packs (e.g. "eng", "chi_sim", "eng+chi_sim")
  lang: "eng"
  # Optional: absolute path to tesseract binary for local dev (Windows).
  # Example: "C:\\Program Files\\Tesseract-OCR\\tesseract.exe"
  # Can also be provided via env var `TESSERACT_CMD`.
  tesseract_cmd: null

multimodal:
  # Multimodal image embedding (used for image search / multimodal recall).
  # Default disabled to keep runtime light.
  enabled: false
  backend: "openclip"
  model: "ViT-B-32"
  pretrained: "laion2b_s34b_b79k"
  device: "cpu"

